{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4582f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터저장소가 이미 있는 상황\n",
    "from dotenv import load_dotenv  # api-key\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a77ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 벡터 스토어 가져오기\n",
    "embedding = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "persist_directory = \"../07_vectorstore/samsung_2025_db\"\n",
    "collection_name = \"samsung2025\"\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    collection_name=collection_name,\n",
    "    embedding_function=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d88f165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. retriever 만들기\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type = \"similarity\",\n",
    "    search_kwargs = {\"k\" : 30}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bbb74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. reranker 만들기\n",
    "from langchain_community.cross_encoders.huggingface import HuggingFaceCrossEncoder\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "\n",
    "hf_ce = HuggingFaceCrossEncoder(\n",
    "    model_name = \"cross-encoder/ms-marco-MiniLM-L6-v2\",\n",
    "    model_kwargs = {\n",
    "        \"device\" : \"cuda\",\n",
    "        \"max_length\" : 512,\n",
    "    }\n",
    ")\n",
    "reranker = CrossEncoderReranker(\n",
    "    model = hf_ce,\n",
    "    top_n = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397141da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. retriever -> reranker\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "com_retriever = ContextualCompressionRetriever(\n",
    "    base_retriever = retriever,\n",
    "    base_compressor = reranker\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bfc8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_transformers import LongContextReorder\n",
    "\n",
    "# 5. reorder(순서 정리)\n",
    "reorder = LongContextReorder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb35b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 검색 결과 문서 합치는 함수 생성\n",
    "def format_docs(docs):\n",
    "    result = []\n",
    "    for item in docs:\n",
    "        result.append(item.page_content)\n",
    "    return \"\\n\\n---\\n\\n\".join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55145592",
   "metadata": {},
   "source": [
    "### 2. 기본 체안 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc5b5a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='주어진 컨텍스트만 근거로 간결하고 정확하게 답하도록 해라. \\n\\n     [컨텍스트]\\n     {context}\\n\\n     '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000021E835A2990>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000021E81F09DD0>, root_client=<openai.OpenAI object at 0x0000021E833CB790>, root_async_client=<openai.AsyncOpenAI object at 0x0000021E82D6A010>, model_name='gpt-4.1-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 프롬프트 설정\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"주어진 컨텍스트만 근거로 간결하고 정확하게 답하도록 해라. \n",
    "     \n",
    "     [컨텍스트]\n",
    "     {context}\n",
    "     \n",
    "     \"\"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# 2. 모델 설정\n",
    "model = ChatOpenAI(\n",
    "    model = \"gpt-4.1-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# 3. outputparser \n",
    "outputparser = StrOutputParser()\n",
    "\n",
    "# 4. 체인 설정\n",
    "chain = rag_prompt | model | outputparser\n",
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5321be27",
   "metadata": {},
   "source": [
    "### 3. 통합 체인 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fb1480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"삼성전자는 인재와 기술을 바탕으로 최고의 제품과 서비스를 창출하여 인류사회에 공헌한다는 경영철학 아래 기술 리더십으로 재도약의 기반을 다지고, 새로운 영역에서 미래 성장동력을 확보해 나갈 계획입니다. 또한, 2025년에는 '삼성 청년SW·AI아카데미' 교육 기회를 마이스터고 졸업생까지 확대하고, '삼성 희망디딤돌' 인천센터를 추가 설립하여 더 많은 청년을 지원할 예정입니다. 환경 분야에서는 2050년 탄소중립 달성을 목표로 온실가스 감축과 재생에너지 전환 등 중장기 전략을 추진하고 있습니다.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain  = (\n",
    "    {\"docs\" : RunnableLambda(lambda x: com_retriever.invoke(x[\"question\"])),\n",
    "     \"question\" : RunnablePassthrough()\n",
    "     }\n",
    "     | RunnableLambda(lambda x : {\n",
    "         \"context\" : format_docs(reorder.transform_documents(x[\"docs\"])),\n",
    "         \"question\" : x['question']\n",
    "     })\n",
    "     | chain\n",
    ")\n",
    "\n",
    "rag_chain.invoke({\n",
    "    \"question\" : \"삼성의 미래 계획은 어떻게 되나요?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f964b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  docs: RunnableLambda(lambda x: com_retriever.invoke(x['question'])),\n",
       "  question: RunnablePassthrough()\n",
       "}\n",
       "| RunnableLambda(...)\n",
       "| ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='주어진 컨텍스트만 근거로 간결하고 정확하게 답하도록 해라. \\n\\n     [컨텍스트]\\n     {context}\\n\\n     '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000021E81BF1E50>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000021E81BF0250>, root_client=<openai.OpenAI object at 0x0000021E81BF1610>, root_async_client=<openai.AsyncOpenAI object at 0x0000021E82196050>, model_name='gpt-4.1-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd79973",
   "metadata": {},
   "source": [
    "### 4. multi_input chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cb20b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_retriever_chain = RunnableLambda(lambda x: x['question']) | com_retriever | format_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcdaf67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'삼성전자는 인재와 기술을 바탕으로 최고의 제품과 서비스를 창출하여 인류사회에 공헌한다는 경영철학 아래 기술 리더십으로 재도약의 기반을 다지고, 새로운 영역에서 미래 성장동력을 확보해 나갈 계획입니다. 또한, 지속가능한 성장 기반 마련을 위해 이해관계자의 의견에 귀 기울이며 지속적으로 노력할 예정입니다.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain = (\n",
    "    {\n",
    "        \"context\" :  com_retriever_chain,\n",
    "        \"question\" : RunnablePassthrough()\n",
    "    }\n",
    "    | chain\n",
    ")\n",
    "rag_chain.invoke({\n",
    "    \"question\" : \"삼성의 미래 계획은 어떻게 되나요?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa4f9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'pro', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='주어진 컨텍스트만 근거로 간결하고 정확하게 답하도록 해라. \\n\\n     [컨텍스트]\\n     {context}\\n\\n     '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['pro', 'question'], input_types={}, partial_variables={}, template='{pro} 스타일에 맞게 {question}에 대답해라'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000021E83594310>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000021E835969D0>, root_client=<openai.OpenAI object at 0x0000021E6FDC5050>, root_async_client=<openai.AsyncOpenAI object at 0x0000021E835966D0>, model_name='gpt-4.1-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 프롬프트 설정\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"주어진 컨텍스트만 근거로 간결하고 정확하게 답하도록 해라. \n",
    "     \n",
    "     [컨텍스트]\n",
    "     {context}\n",
    "     \n",
    "     \"\"\"),\n",
    "    (\"human\", \"{pro} 스타일에 맞게 {question}에 대답해라\")\n",
    "])\n",
    "\n",
    "# 2. 모델 설정\n",
    "MODEL_NAME =  \"gpt-4.1-mini\"\n",
    "temperature = 0\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model = MODEL_NAME,\n",
    "    temperature = temperature\n",
    ")\n",
    "\n",
    "# 3. outputparser \n",
    "outputparser = StrOutputParser()\n",
    "\n",
    "# 4. 체인 설정\n",
    "chain = rag_prompt | model | outputparser\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f33c848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c026af42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'삼성은 인재와 기술을 바탕으로 최고의 제품과 서비스를 만들어 인류사회에 공헌할 계획이냥! 기술 리더십으로 재도약하고 새로운 영역에서 미래 성장동력을 확보할 거라냥! 지속가능한 성장 기반 마련에도 최선을 다할 예정이냥!'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 프롬프트 설정\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"주어진 컨텍스트만 근거로 간결하고 정확하게 답하도록 해라. \n",
    "     \n",
    "     [컨텍스트]\n",
    "     {context}\n",
    "     \n",
    "     \"\"\"),\n",
    "    (\"human\", \"{pro} 스타일에 맞게 {question}에 대답해라\")\n",
    "])\n",
    "\n",
    "# 2. 모델 설정\n",
    "MODEL_NAME =  \"gpt-4.1-mini\"\n",
    "temperature = 0\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model = MODEL_NAME,\n",
    "    temperature = temperature\n",
    ")\n",
    "\n",
    "# 3. outputparser \n",
    "outputparser = StrOutputParser()\n",
    "\n",
    "# 4. 체인 설정\n",
    "chain = rag_prompt | model | outputparser\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\" :  RunnableLambda(lambda x: x['question']) | com_retriever | format_docs,\n",
    "        \"question\" : RunnablePassthrough(),\n",
    "        \"pro\" : RunnableLambda(lambda x: x['pro'])\n",
    "    }\n",
    "    | chain\n",
    ")\n",
    "\n",
    "# 실행 코드\n",
    "rag_chain.invoke({\n",
    "    \"question\" : \"삼성의 미래 계획은 어떻게 되나요?\",\n",
    "    \"pro\" : \"냥냥체\"\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "10_LangChain-Study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
